pg = {
  dataSourceClass = org.postgresql.ds.PGSimpleDataSource
  properties = {
    serverName = "localhost"
    databaseName = "sqltest"
    user = "super"
    password = "aszx"
  }
  numThreads = 10
}

akka.kafka.producer {
  # Tuning parameter of how many sends that can run in parallel.
  parallelism = 10000
  kafka-clients {
    bootstrap.servers = "127.0.0.1:9092"
    acks = "0"
    retries = 5
    batch.size = 16384
    linger.ms = 1
    buffer.memory = 33554432
  }
}

# 消费 Kafka 消息的配置
akka.kafka.consumer {
  kafka-clients {
    # Kafka 的主机名字符串, 多个之间用逗号分开
    bootstrap.servers = "127.0.0.1:9092"
    # group id, 非常非常重要
    group.id = "gg_es"
    # 如果没有存储的 offset, 那么就默认从头开始消费
    auto.offset.reset = "latest"
  }
}

kafka {
  # 需要消费的kafka topic
  topic-in = "k3"
  # 目标 topic
  topic-out = "k3"
  # 分配的 partition 列表. 如果置为空([]), 将会分配所有的 partition
  partitions = []
  commit {
    # 每批消费的消息数目
    batch-size = 100000
    # 每批消费的最大时间窗
    batch-time-window = 5.seconds
  }
}

kafka.topic-out = "adx-logs"

kafka.logs.topic-out = "k3"

aerospike {

  cluster {
    main {
      hosts = ["adx61:3000", "adx62:3000"]
      timeout = 100.milliseconds
    }
  }

  test-main {
    cluster = ${aerospike.cluster.main}
    namespace = "adx_main"
    set1 = "set1"
    set2 = "set2"
    # 相当与字段
    bin = "cache"
    expire = 10.minutes
  }


}